#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
éªŒè¯ resolve_innovation_duplicates ä¸­çš„ä¿¡æ¯ä¿ç•™æƒ…å†µ
"""

import pandas as pd
import sys

def create_test_data():
    """åˆ›å»ºæµ‹è¯•æ•°æ® - æ¨¡æ‹ŸåŒä¸€åˆ›æ–°æœ‰å¤šæ¡å…³ç³»è®°å½•çš„æƒ…å†µ"""
    return pd.DataFrame({
        'source_type': ['Innovation'] * 6,
        'source_id': ['I001', 'I001', 'I001', 'I002', 'I002', 'I002'],
        'source_english_id': ['AI Platform'] * 3 + ['ML Engine'] * 3,
        'source_description': ['Advanced AI system'] * 3 + ['Machine learning engine'] * 3,
        'relationship_type': ['DEVELOPED_BY', 'USES', 'ENABLES', 'DEVELOPED_BY', 'USES', 'COLLABORATION'],
        'target_id': ['O001', 'S001', 'T001', 'O002', 'S002', 'O003'],
        'target_english_id': ['TechCorp', 'Cloud Service', 'Analytics Tool', 'DataLab', 'GPU Cluster', 'Research Institute'],
        'target_description': ['Technology company', 'Cloud infrastructure', 'Analytics platform', 'Data laboratory', 'GPU hardware', 'Research org'],
        'relationship description': ['', 'Utilizes', 'Enables', '', 'Uses', 'Collaborates with'],
        'data_source': ['company_website'] * 3 + ['vtt_website'] * 3
    })

def test_deduplication_information_loss():
    """æµ‹è¯•å»é‡æ˜¯å¦ä¸¢å¤±ä¿¡æ¯"""
    print("=" * 80)
    print("æµ‹è¯• 1: å»é‡æ“ä½œçš„ä¿¡æ¯ä¿ç•™æƒ…å†µ")
    print("=" * 80)
    
    df = create_test_data()
    
    print(f"\nğŸ“Š åŸå§‹æ•°æ®: {len(df)} è¡Œ")
    print(df[['source_id', 'relationship_type', 'target_english_id']].to_string(index=False))
    
    # æ‰§è¡Œå»é‡
    innovations = df[df["source_type"] == "Innovation"]
    unique_innovations = innovations.drop_duplicates(subset=["source_id"]).reset_index(drop=True)
    
    print(f"\nğŸ“‰ å»é‡å: {len(unique_innovations)} è¡Œ")
    print(unique_innovations[['source_id', 'relationship_type', 'target_english_id']].to_string(index=False))
    
    # ç»Ÿè®¡ä¿¡æ¯ä¸¢å¤±
    original_relations = set()
    for _, row in df.iterrows():
        original_relations.add((row['source_id'], row['relationship_type'], row['target_english_id']))
    
    kept_relations = set()
    for _, row in unique_innovations.iterrows():
        kept_relations.add((row['source_id'], row['relationship_type'], row['target_english_id']))
    
    lost_relations = original_relations - kept_relations
    
    print(f"\nâŒ ä¸¢å¤±çš„å…³ç³» ({len(lost_relations)} æ¡):")
    for rel in sorted(lost_relations):
        print(f"   - {rel[0]}: {rel[1]} â†’ {rel[2]}")
    
    return df, unique_innovations

def test_feature_building_information_usage():
    """æµ‹è¯•ç‰¹å¾æ„å»ºæ˜¯å¦ä½¿ç”¨äº†å®Œæ•´ä¿¡æ¯"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 2: ç‰¹å¾æ„å»ºæ—¶çš„ä¿¡æ¯ä½¿ç”¨æƒ…å†µ")
    print("=" * 80)
    
    from data.processors import InnovationFeatureBuilder
    
    df, unique_innovations = create_test_data(), create_test_data().drop_duplicates(subset=["source_id"])
    
    print(f"\næ„å»ºç‰¹å¾...")
    
    for _, row in unique_innovations.iterrows():
        innovation_id = row['source_id']
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = InnovationFeatureBuilder.build_context(row, df)
        
        print(f"\nğŸ“ åˆ›æ–° {innovation_id}:")
        print(f"   ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
        print(f"   ä¸Šä¸‹æ–‡å†…å®¹: {context[:150]}...")
        
        # ç»Ÿè®¡ä½¿ç”¨äº†å¤šå°‘æ¡å…³ç³»
        related_rows = df[df['source_id'] == innovation_id]
        print(f"   âœ… ä½¿ç”¨äº† {len(related_rows)} æ¡å…³ç³»è®°å½• (åŸå§‹æ•°æ®ä¸­çš„æ‰€æœ‰è®°å½•)")
        
        # æ£€æŸ¥æ¯ä¸ªå…³ç³»æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
        for _, rel_row in related_rows.iterrows():
            target_name = rel_row['target_english_id']
            if target_name in context:
                print(f"      âœ“ {rel_row['relationship_type']:15s} â†’ {target_name:20s} [å·²åŒ…å«]")
            else:
                print(f"      âœ— {rel_row['relationship_type']:15s} â†’ {target_name:20s} [æœªåŒ…å«]")

def test_clustering_with_complete_features():
    """æµ‹è¯•å®Œæ•´æµç¨‹ï¼šå»é‡ + ç‰¹å¾æ„å»º + èšç±»"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 3: å®Œæ•´æµç¨‹éªŒè¯")
    print("=" * 80)
    
    from data.processors import (
        InnovationExtractor,
        InnovationFeatureBuilder,
        EmbeddingManager
    )
    
    # åˆ›å»ºæ›´å¤æ‚çš„æµ‹è¯•æ•°æ®
    df = pd.DataFrame({
        'source_type': ['Innovation'] * 9,
        'source_id': ['I001', 'I001', 'I001', 'I002', 'I002', 'I003', 'I003', 'I003', 'I003'],
        'source_english_id': ['AI Platform'] * 3 + ['AI System'] * 2 + ['AI Platform'] * 4,  # I001 å’Œ I003 åç§°ç›¸åŒ
        'source_description': ['AI tech'] * 3 + ['AI technology'] * 2 + ['AI tech'] * 4,
        'relationship_type': ['DEVELOPED_BY', 'USES', 'ENABLES', 'DEVELOPED_BY', 'USES', 'DEVELOPED_BY', 'USES', 'ENABLES', 'COLLABORATION'],
        'target_id': ['O001', 'S001', 'T001', 'O002', 'S002', 'O001', 'S001', 'T001', 'O003'],
        'target_english_id': ['TechCorp', 'Cloud', 'Analytics', 'DataLab', 'GPU', 'TechCorp', 'Cloud', 'Analytics', 'Institute'],
        'target_description': ['Company'] * 3 + ['Lab', 'Hardware'] + ['Company', 'Service', 'Tool', 'Research'],
        'relationship description': [''] * 9,
        'data_source': ['company_website'] * 9
    })
    
    print(f"\nåŸå§‹æ•°æ®: {len(df)} è¡Œï¼Œæ¶‰åŠ {df['source_id'].nunique()} ä¸ªå”¯ä¸€åˆ›æ–°")
    
    # Step 1: æå–å”¯ä¸€åˆ›æ–°
    unique = InnovationExtractor.extract_unique_innovations(df)
    print(f"å»é‡å: {len(unique)} è¡Œ")
    
    # Step 2: æ„å»ºç‰¹å¾
    features = InnovationFeatureBuilder.build_all_features(unique, df, show_progress=False)
    
    print(f"\næ„å»ºçš„ç‰¹å¾:")
    for iid, context in features.items():
        relation_count = len(df[df['source_id'] == iid])
        print(f"\n{iid}: ä½¿ç”¨äº† {relation_count} æ¡å…³ç³»")
        print(f"   ä¸Šä¸‹æ–‡: {context[:100]}...")
        
        # æ£€æŸ¥å…³é”®ä¿¡æ¯
        developers = df[(df['source_id'] == iid) & (df['relationship_type'] == 'DEVELOPED_BY')]['target_english_id'].tolist()
        for dev in developers:
            if dev in context:
                print(f"   âœ“ åŒ…å«å¼€å‘è€…: {dev}")
            else:
                print(f"   âœ— ç¼ºå¤±å¼€å‘è€…: {dev}")

def test_graph_clustering_usage():
    """æµ‹è¯•å›¾èšç±»æ˜¯å¦å¯ç”¨"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 4: å›¾èšç±»æ–¹æ³•éªŒè¯")
    print("=" * 80)
    
    from data.processors import ClusteringStrategyFactory
    
    methods = [
        ('hdbscan', 'å¹³é¢èšç±»'),
        ('kmeans', 'å¹³é¢èšç±»'),
        ('graph_threshold', 'å›¾èšç±»'),
        ('graph_kcore', 'å›¾èšç±»'),
    ]
    
    print("\næ”¯æŒçš„èšç±»æ–¹æ³•:")
    for method, category in methods:
        try:
            strategy = ClusteringStrategyFactory.create_strategy(method)
            print(f"   âœ… {method:20s} [{category:10s}] - {type(strategy).__name__}")
        except Exception as e:
            print(f"   âŒ {method:20s} - å¤±è´¥: {e}")
    
    print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
    print("   # ä½¿ç”¨é»˜è®¤çš„ HDBSCAN")
    print("   resolve_innovation_duplicates(df, method='hdbscan')")
    print("\n   # ä½¿ç”¨å›¾èšç±»")
    print("   resolve_innovation_duplicates(df, method='graph_threshold', similarity_threshold=0.85)")

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\nğŸ” éªŒè¯ resolve_innovation_duplicates çš„ä¿¡æ¯ä¿ç•™æœºåˆ¶\n")
    
    # æµ‹è¯•1: å»é‡çš„ä¿¡æ¯ä¸¢å¤±
    test_deduplication_information_loss()
    
    # æµ‹è¯•2: ç‰¹å¾æ„å»ºçš„ä¿¡æ¯ä½¿ç”¨
    test_feature_building_information_usage()
    
    # æµ‹è¯•3: å®Œæ•´æµç¨‹
    test_clustering_with_complete_features()
    
    # æµ‹è¯•4: å›¾èšç±»
    test_graph_clustering_usage()
    
    print("\n" + "=" * 80)
    print("ğŸ“‹ æ€»ç»“:")
    print("=" * 80)
    print("""
1. âŒ drop_duplicates(subset=['source_id']) ç¡®å®ä¼šä¸¢å¤±ä¿¡æ¯
   - åªä¿ç•™æ¯ä¸ªåˆ›æ–°çš„ç¬¬ä¸€æ¡è®°å½•
   - å…¶ä»–å…³ç³»è®°å½•è¢«ä¸¢å¼ƒ

2. âœ… ä½†ç‰¹å¾æ„å»ºæ—¶ä¼šå›æŸ¥å®Œæ•´çš„ df_relationships
   - InnovationFeatureBuilder.build_context() ä½¿ç”¨å®Œæ•´ DataFrame
   - æ‰€æœ‰å…³ç³»ä¿¡æ¯éƒ½ä¼šè¢«åŒ…å«åœ¨æ–‡æœ¬ä¸Šä¸‹æ–‡ä¸­

3. âœ… å›¾èšç±»æ–¹æ³•å¯ç”¨
   - graph_threshold: åŸºäºç›¸ä¼¼åº¦é˜ˆå€¼æ„å»ºå›¾
   - graph_kcore: åŸºäº K-core åˆ†è§£

4. ğŸ’¡ æ”¹è¿›å»ºè®®:
   - å»é‡æ—¶å¯ä»¥èšåˆä¿¡æ¯ï¼Œé¿å…è¯¯è§£
   - ä½¿ç”¨ groupby é¢„å¤„ç†å¯ä»¥æé«˜æ•ˆç‡
   - æ˜¾å¼ä¿å­˜å…³ç³»ç»Ÿè®¡ä¿¡æ¯
    """)
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
