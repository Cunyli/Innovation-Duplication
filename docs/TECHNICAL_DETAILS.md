# æŠ€æœ¯ç»†èŠ‚æ–‡æ¡£

> é«˜å±‚ç»“æ„å’ŒåŒ…æ¦‚è§ˆè¯·å…ˆé˜…è¯» **DEVELOPMENT.md**ã€‚æœ¬æ–‡ä»¶ä¿ç•™æ·±å…¥å®ç°ç»†èŠ‚å’Œå…¬å¼ã€‚

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜åˆ›æ–°å»é‡ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯å®ç°ã€‚

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š`resolve_innovation_duplicates`

è¿™æ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒæ–¹æ³•ï¼Œç”¨äºè¯†åˆ«å’Œåˆå¹¶é‡å¤çš„åˆ›æ–°ã€‚

### é»˜è®¤æ‰§è¡Œæµç¨‹

```python
# åœ¨ main() å‡½æ•°ä¸­è°ƒç”¨
canonical_mapping = resolve_innovation_duplicates(
    df_relationships=df_relationships,  # è¾“å…¥ï¼šå…³ç³»æ•°æ®
    model=embed_model,                  # Azure OpenAI åµŒå…¥æ¨¡å‹
    cache_config=cache_config,          # ç¼“å­˜é…ç½®
    method="hdbscan",                   # é»˜è®¤èšç±»æ–¹æ³•
    min_cluster_size=2,
    metric="cosine",
    cluster_selection_method="eom"
)
```

## ğŸ“Š æ•°æ®æµç¨‹

### å®Œæ•´æ•°æ®æµå›¾

```
åŸå§‹æ–‡ä»¶ (CSV + PKL)
    â†“
load_and_combine_data()
    â†“
df_relationships (å®Œæ•´å…³ç³»è¡¨ï¼Œ~1000è¡Œ)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ source_id â”‚ source_type â”‚ relationship_type â”‚... â”‚
â”‚ I001      â”‚ Innovation  â”‚ DEVELOPED_BY      â”‚... â”‚
â”‚ I001      â”‚ Innovation  â”‚ USES              â”‚... â”‚
â”‚ I001      â”‚ Innovation  â”‚ ENABLES           â”‚... â”‚
â”‚ I002      â”‚ Innovation  â”‚ DEVELOPED_BY      â”‚... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
resolve_innovation_duplicates()
    â”‚
    â”œâ”€ Step 1: extract_unique_innovations()
    â”‚  â””â”€ æå–å”¯ä¸€ID (å»é‡)
    â”‚     è¾“å…¥: 1000è¡Œ â†’ è¾“å‡º: 200ä¸ªå”¯ä¸€åˆ›æ–°ID
    â”‚
    â”œâ”€ Step 2: build_all_features()
    â”‚  â””â”€ æ„å»ºåˆ›æ–°ç‰¹å¾ï¼ˆå›æŸ¥å®Œæ•´ DataFrameï¼‰
    â”‚     ä¸ºæ¯ä¸ªåˆ›æ–°æ„å»ºæ–‡æœ¬ï¼š
    â”‚     "Innovation name: ... Description: ...
    â”‚      Developed by: ... Relationships: ..."
    â”‚
    â”œâ”€ Step 3: get_embeddings()
    â”‚  â””â”€ ç”Ÿæˆæˆ–åŠ è½½åµŒå…¥å‘é‡
    â”‚     - æ£€æŸ¥ç¼“å­˜
    â”‚     - è°ƒç”¨ Azure OpenAI API
    â”‚     - ä¿å­˜åˆ°ç¼“å­˜
    â”‚     è¾“å‡º: (200, 1536) åµŒå…¥çŸ©é˜µ
    â”‚
    â””â”€ Step 4: cluster()
       â””â”€ æ‰§è¡Œèšç±»ç®—æ³•
          è¾“å…¥: åµŒå…¥çŸ©é˜µ
          è¾“å‡º: canonical_mapping
          {
            'I001': 'I001',
            'I023': 'I001',  # é‡å¤ï¼Œåˆå¹¶åˆ° I001
            'I045': 'I001',  # é‡å¤ï¼Œåˆå¹¶åˆ° I001
            'I002': 'I002',
            ...
          }
```

## ğŸ” è¯¦ç»†æ­¥éª¤è§£æ

### Step 1: æå–å”¯ä¸€åˆ›æ–°

```python
# åœ¨ data/processors/innovation_feature_builder.py
def extract_unique_innovations(df_relationships: pd.DataFrame):
    # 1. ç­›é€‰åˆ›æ–°ç±»å‹
    innovations = df_relationships[
        df_relationships["source_type"] == "Innovation"
    ]
    
    # 2. æŒ‰ source_id å»é‡
    unique_innovations = innovations.drop_duplicates(subset=["source_id"])
    
    return unique_innovations  # 200è¡Œ
```

**é‡è¦ç†è§£ï¼š**
- è™½ç„¶å»é‡ååªæœ‰ 200 è¡Œ
- ä½†åŸå§‹çš„ `df_relationships` ä»ç„¶ä¿ç•™å®Œæ•´çš„ 1000 è¡Œ
- åç»­æ­¥éª¤ä¼šå›æŸ¥è¿™ 1000 è¡Œæ¥æ„å»ºç‰¹å¾

### Step 2: æ„å»ºåˆ›æ–°ç‰¹å¾

```python
# åœ¨ data/processors/innovation_feature_builder.py
def build_all_features(unique_innovations, df_relationships):
    innovation_features = {}
    
    for _, row in unique_innovations.iterrows():
        innovation_id = row["source_id"]  # ä¾‹å¦‚ "I001"
        
        # ğŸ”‘ å…³é”®ï¼šè°ƒç”¨ build_contextï¼Œä¼ å…¥å®Œæ•´çš„ df_relationships
        context = build_context(row, df_relationships)
        innovation_features[innovation_id] = context
    
    return innovation_features

def build_context(innovation_row, df_relationships):
    innovation_id = innovation_row["source_id"]
    
    # 1. åŸºæœ¬ä¿¡æ¯
    context = f"Innovation name: {innovation_row['source_english_id']}. "
    context += f"Description: {innovation_row['source_description']}. "
    
    # 2. å¼€å‘è€…ä¿¡æ¯ï¼ˆæŸ¥è¯¢å®Œæ•´ DataFrameï¼‰
    developers = df_relationships[
        (df_relationships["source_id"] == innovation_id) &
        (df_relationships["relationship_type"] == "DEVELOPED_BY")
    ]["target_english_id"].unique()
    
    context += f"Developed by: {', '.join(developers)}. "
    
    # 3. å…³ç³»ä¸Šä¸‹æ–‡ï¼ˆæŸ¥è¯¢å®Œæ•´ DataFrameï¼‰
    relations = df_relationships[
        df_relationships["source_id"] == innovation_id
    ]
    
    for _, rel_row in relations.iterrows():
        context += f"{rel_row['relationship_type']} {rel_row['target_english_id']}. "
    
    return context
```

**å®Œæ•´ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼š**

```python
innovation_features = {
    'I001': """Innovation name: AI Platform. 
               Description: Advanced AI system. 
               Developed by: Fortum, Nokia. 
               USES Cloud Service. 
               ENABLES Analytics Tool.""",
    'I002': """Innovation name: ML Engine. 
               Description: Machine learning tool. 
               Developed by: Nokia. 
               USES GPU Cluster.""",
    # ... 200 ä¸ªåˆ›æ–°
}
```

### Step 3: ç”ŸæˆåµŒå…¥å‘é‡

```python
# åœ¨ data/processors/embedding_manager.py
def get_embeddings(self, innovation_features, model):
    # 1. ä»ç¼“å­˜åŠ è½½
    embeddings = self.cache.load()  # ä» embedding_vectors.json
    
    # 2. æ‰¾å‡ºç¼ºå¤±çš„ ID
    all_ids = list(innovation_features.keys())
    missing_ids = self.cache.get_missing_keys(all_ids)
    
    # 3. ä¸ºç¼ºå¤±çš„åˆ›æ–°ç”ŸæˆåµŒå…¥
    if missing_ids:
        print(f"Generating {len(missing_ids)} new embeddings...")
        
        new_embeddings = {}
        for innovation_id in tqdm(missing_ids):
            text = innovation_features[innovation_id]
            
            # è°ƒç”¨ Azure OpenAI API
            embedding = get_embedding(text, model)
            # è¿”å›: [0.15, 0.22, 0.31, ..., 0.87]  # 1536ç»´
            
            new_embeddings[innovation_id] = embedding
        
        # 4. æ›´æ–°ç¼“å­˜
        self.cache.update(new_embeddings)
        embeddings.update(new_embeddings)
    
    # 5. æ„å»ºåµŒå…¥çŸ©é˜µ
    innovation_ids = list(embeddings.keys())
    embedding_matrix = np.array([embeddings[id] for id in innovation_ids])
    # shape: (200, 1536)
    
    return innovation_ids, embedding_matrix
```

**åµŒå…¥å‘é‡çš„å«ä¹‰ï¼š**
- æ¯ä¸ªåˆ›æ–°çš„æ–‡æœ¬ â†’ 1536 ç»´æ•°å­—å‘é‡
- ç›¸ä¼¼çš„åˆ›æ–°åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘
- ä½¿ç”¨ Azure OpenAI çš„ text-embedding-ada-002 æ¨¡å‹

### Step 4: æ‰§è¡Œèšç±»

```python
# åœ¨ data/processors/clustering_strategy.py
class FlatClusteringStrategy:
    def cluster(self, embedding_matrix, innovation_ids, **kwargs):
        # 1. è°ƒç”¨ HDBSCAN
        labels = cluster_hdbscan(
            embedding_matrix=embedding_matrix,  # (200, 1536)
            min_cluster_size=2,
            metric="cosine"
        )
        
        # 2. HDBSCAN è¿”å›æ ‡ç­¾
        # labels = [0, 0, 0, 1, 1, 2, -1, 3, 3, 3, ...]
        #          ^^^^^^     ^^^ ^^^  ^  ^^^^^^^^^
        #          ç°‡0        ç°‡1 ç°‡2 å™ªéŸ³  ç°‡3
        
        # 3. æ„å»ºç°‡å­—å…¸
        clusters = {}
        for idx, label in enumerate(labels):
            if label == -1:
                # å™ªéŸ³ç‚¹å•ç‹¬æˆç°‡
                clusters[f"noise_{innovation_ids[idx]}"] = [innovation_ids[idx]]
            else:
                clusters.setdefault(int(label), []).append(innovation_ids[idx])
        
        # 4. è½¬æ¢ä¸ºæ ‡å‡†æ˜ å°„
        canonical_mapping = {}
        for label_key, members in clusters.items():
            canonical_id = members[0]  # ç¬¬ä¸€ä¸ªä½œä¸ºä»£è¡¨
            for member_id in members:
                canonical_mapping[member_id] = canonical_id
        
        return canonical_mapping
```

## ğŸ”„ ä¿¡æ¯ä¿ç•™æœºåˆ¶

### å¸¸è§ç–‘é—®ï¼šå»é‡ä¼šä¸¢å¤±ä¿¡æ¯å—ï¼Ÿ

**ç­”æ¡ˆï¼šä¸ä¼šï¼** è™½ç„¶ `drop_duplicates` çœ‹èµ·æ¥ä¸¢å¤±äº†è¡Œï¼Œä½†å®é™…ä¸Šï¼š

```python
# çœ‹èµ·æ¥ä¸¢å¤±äº†ä¿¡æ¯
unique_innovations = df.drop_duplicates(subset=["source_id"])
# 1000 è¡Œ â†’ 200 è¡Œ

# ä½†ç‰¹å¾æ„å»ºæ—¶ä¼šå›æŸ¥å®Œæ•´ DataFrame
def build_context(innovation_row, df_relationships):  # â† ä¼ å…¥å®Œæ•´çš„ 1000 è¡Œ
    innovation_id = innovation_row["source_id"]
    
    # æŸ¥è¯¢æ‰€æœ‰ä¸è¯¥åˆ›æ–°ç›¸å…³çš„è¡Œ
    relations = df_relationships[
        df_relationships["source_id"] == innovation_id
    ]  # â† å¯èƒ½æŸ¥åˆ°å¤šè¡Œ
    
    # æ‰€æœ‰å…³ç³»ä¿¡æ¯éƒ½ä¼šè¢«ä½¿ç”¨
    for _, rel_row in relations.iterrows():
        context += f"{rel_row['relationship_type']} {rel_row['target_name']}. "
```

**éªŒè¯ç¤ºä¾‹ï¼š**

```python
# å‡è®¾ I001 æœ‰ 3 æ¡å…³ç³»è®°å½•
df_relationships:
  source_id  relationship_type  target_name
  I001       DEVELOPED_BY       Fortum
  I001       USES               Cloud
  I001       ENABLES            Analytics

# Step 1: å»é‡å
unique_innovations:
  source_id  ...
  I001       ...  # åªä¿ç•™ 1 è¡Œ

# Step 2: æ„å»ºç‰¹å¾æ—¶
context = build_context(I001, df_relationships)
# å›æŸ¥å®Œæ•´ DataFrameï¼Œæ‰¾åˆ° I001 çš„ 3 æ¡è®°å½•
# ç»“æœ: "... Developed by Fortum. Uses Cloud. Enables Analytics."

# âœ… æ‰€æœ‰ 3 æ¡å…³ç³»éƒ½è¢«ä¿ç•™åœ¨æ–‡æœ¬ä¸­ï¼
```

## ğŸ“ æ¶‰åŠçš„æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒä»£ç æ–‡ä»¶

```
innovation_resolution.py                # ä¸»æµç¨‹
â”œâ”€â”€ load_and_combine_data()
â”œâ”€â”€ resolve_innovation_duplicates()    # æ ¸å¿ƒæ–¹æ³•
â”œâ”€â”€ create_innovation_knowledge_graph()
â””â”€â”€ analyze_innovation_network()

data/processors/
â”œâ”€â”€ innovation_feature_builder.py      # ç‰¹å¾æ„å»º
â”‚   â”œâ”€â”€ InnovationExtractor
â”‚   â””â”€â”€ InnovationFeatureBuilder
â”œâ”€â”€ embedding_manager.py               # åµŒå…¥ç®¡ç†
â”‚   â””â”€â”€ EmbeddingManager
â”œâ”€â”€ clustering_strategy.py             # èšç±»ç­–ç•¥
â”‚   â”œâ”€â”€ FlatClusteringStrategy
â”‚   â”œâ”€â”€ GraphClusteringStrategy
â”‚   â””â”€â”€ ClusteringStrategyFactory
â”œâ”€â”€ embedding_strategy.py              # åµŒå…¥ç”Ÿæˆ
â”‚   â””â”€â”€ get_embedding()
â””â”€â”€ model_initializer.py              # æ¨¡å‹åˆå§‹åŒ–
    â””â”€â”€ initialize_openai_client()

utils/cluster/
â”œâ”€â”€ cluster_algorithms.py             # å¹³é¢èšç±»ç®—æ³•
â””â”€â”€ graph_clustering.py               # å›¾èšç±»ç®—æ³•

core/
â””â”€â”€ cache.py                          # ç¼“å­˜ç³»ç»Ÿ
    â”œâ”€â”€ CacheBackend
    â”œâ”€â”€ JsonFileCache
    â”œâ”€â”€ MemoryCache
    â””â”€â”€ EmbeddingCache
```

### æ•°æ®æ–‡ä»¶

```
data/
â”œâ”€â”€ dataframes/
â”‚   â”œâ”€â”€ vtt_mentions_comp_domain.csv     # å…¬å¸ç½‘ç«™æ•°æ®
â”‚   â””â”€â”€ comp_mentions_vtt_domain.csv     # VTT ç½‘ç«™æ•°æ®
â”‚
â””â”€â”€ graph_docs_names_resolved/           # å›¾æ–‡æ¡£ï¼ˆPKL æ–‡ä»¶ï¼‰
    â”œâ”€â”€ Fortum Oyj_0.pkl
    â”œâ”€â”€ Nokia Oyj_0.pkl
    â””â”€â”€ ...

embedding_vectors.json                   # åµŒå…¥å‘é‡ç¼“å­˜
```

## ğŸ¯ å…³é”®ç†è§£ç‚¹

### 1. å»é‡ä¸ä¼šä¸¢å¤±ä¿¡æ¯

- `drop_duplicates` åªæ˜¯è·å–å”¯ä¸€ ID åˆ—è¡¨
- ç‰¹å¾æ„å»ºæ—¶ä¼šå›æŸ¥å®Œæ•´ DataFrame
- æ‰€æœ‰å…³ç³»éƒ½ä¼šè¢«ä½¿ç”¨

### 2. é»˜è®¤ä½¿ç”¨å¹³é¢èšç±»

- æ–¹æ³•ï¼šHDBSCAN
- åŸå› ï¼šè‡ªåŠ¨ã€å‡†ç¡®ã€é«˜æ•ˆ
- ä¸ä½¿ç”¨å›¾èšç±»ï¼ˆé™¤éæ˜¾å¼æŒ‡å®šï¼‰

### 3. åµŒå…¥å‘é‡æœ‰ç¼“å­˜

- ç¬¬ä¸€æ¬¡è¿è¡Œï¼šè°ƒç”¨ APIï¼Œä¿å­˜åˆ° `embedding_vectors.json`
- åç»­è¿è¡Œï¼šä»ç¼“å­˜åŠ è½½ï¼Œåªä¸ºæ–°åˆ›æ–°ç”ŸæˆåµŒå…¥
- èŠ‚çœæ—¶é—´å’Œ API è°ƒç”¨è´¹ç”¨

### 4. è¾“å‡ºæ˜¯æ˜ å°„å­—å…¸

```python
canonical_mapping = {
    'I001': 'I001',  # æ ‡å‡†ID
    'I023': 'I001',  # é‡å¤ï¼Œåˆå¹¶åˆ° I001
    'I045': 'I001',  # é‡å¤ï¼Œåˆå¹¶åˆ° I001
    'I002': 'I002',  # æ ‡å‡†ID
    'I015': 'I002',  # é‡å¤ï¼Œåˆå¹¶åˆ° I002
}

# 200 ä¸ªåŸå§‹åˆ›æ–° â†’ 150 ä¸ªå”¯ä¸€åˆ›æ–°
```

## ğŸ”§ é…ç½®é€‰é¡¹

### ç¼“å­˜é…ç½®

```python
cache_config = {
    "type": "embedding",
    "backend": "json",              # 'json' æˆ– 'memory'
    "path": "./embedding_vectors.json",
    "use_cache": True               # æ˜¯å¦å¯ç”¨ç¼“å­˜
}
```

### èšç±»æ–¹æ³•é€‰æ‹©

```python
# HDBSCANï¼ˆæ¨èï¼‰
method="hdbscan"
min_cluster_size=2
metric="cosine"

# K-means
method="kmeans"
n_clusters=450

# å›¾èšç±»ï¼ˆä¸æ¨èï¼‰
method="graph_threshold"
similarity_threshold=0.85
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [èšç±»æ–¹æ³•æŒ‡å—](CLUSTERING_GUIDE.md) - è¯¦ç»†çš„èšç±»æ–¹æ³•è¯´æ˜
- [å¼€å‘æŒ‡å—](DEVELOPMENT.md) - å¼€å‘å’Œè°ƒè¯•æŒ‡å—
- [æ•°æ®ç®¡é“é‡æ„æŒ‡å—](DATA_PIPELINE_REFACTORING_GUIDE.md) - æ¶æ„è®¾è®¡
